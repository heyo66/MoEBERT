{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fda9e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/bert24/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/MoEBERT/src/convert_dataset.py\", line 439, in <module>\n",
      "    main(parse_args())\n",
      "  File \"/home/user/MoEBERT/src/convert_dataset.py\", line 396, in main\n",
      "    dataset = build_hf_dataset(\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/MoEBERT/src/convert_dataset.py\", line 291, in build_hf_dataset\n",
      "    raise ValueError(tok_error_msg)\n",
      "ValueError: This tokenizer does not insert an EOS nor BOS token. Concatenating with this tokenizer will result in sequences being attached without a separating token. Please use another tokenizer, such as facebook/opt-125m, or specify EOS/BOS text with e.g. --bos_text=<|endoftext|>.\n"
     ]
    }
   ],
   "source": [
    "!python src/convert_dataset.py \\\n",
    "  --dataset c4 \\\n",
    "  --data_subset en \\\n",
    "  --splits train_small val_small \\\n",
    "  --out_root ./c4_tokenized_bert_small \\\n",
    "  --concat_tokens 512 \\\n",
    "  --tokenizer bert-base-uncased \\\n",
    "  --bos_text \"[CLS]\" \\\n",
    "  --eos_text \"[SEP]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09965d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/bert24/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/home/user/.cache/huggingface/modules/datasets_modules/datasets/c4/584d57ebe81c209b6c7f31727066d2c4b4bba37cb7092cdd83083d5ec11207db/c4.py:53: FutureWarning: Dataset 'c4' is deprecated and will be deleted. Use 'allenai/c4' instead.\n",
      "  warnings.warn(\n",
      "/home/user/MoEBERT/src/convert_dataset.py:227: UserWarning: The provided tokenizer adds special tokens, but you also specified both eos and bos. This may result in duplicated special tokens. Please be sure this is what you intend.\n",
      "  warnings.warn(\n",
      "/home/user/miniconda3/envs/bert24/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 32, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "Converting train_small to MDS format...\n",
      "train_small: 100%|████████████████████| 100000/100000 [00:30<00:00, 3255.12it/s]\n",
      "/home/user/.cache/huggingface/modules/datasets_modules/datasets/c4/584d57ebe81c209b6c7f31727066d2c4b4bba37cb7092cdd83083d5ec11207db/c4.py:53: FutureWarning: Dataset 'c4' is deprecated and will be deleted. Use 'allenai/c4' instead.\n",
      "  warnings.warn(\n",
      "/home/user/MoEBERT/src/convert_dataset.py:227: UserWarning: The provided tokenizer adds special tokens, but you also specified both eos and bos. This may result in duplicated special tokens. Please be sure this is what you intend.\n",
      "  warnings.warn(\n",
      "Converting val_small to MDS format...\n",
      "val_small: 100%|████████████████████████| 10000/10000 [00:03<00:00, 2612.44it/s]\n"
     ]
    }
   ],
   "source": [
    "!python src/convert_dataset.py \\\n",
    "  --dataset c4 \\\n",
    "  --data_subset en \\\n",
    "  --splits train_small val_small \\\n",
    "  --out_root ./c4_tokenized_bert_small \\\n",
    "  --concat_tokens 512 \\\n",
    "  --tokenizer bert-base-uncased \\\n",
    "  --bos_text \"[CLS]\" \\\n",
    "  --eos_text \"[SEP]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82e9d8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user/.cache/huggingface/modules/datasets_modules/datasets/c4/584d57ebe81c209b6c7f31727066d2c4b4bba37cb7092cdd83083d5ec11207db/c4.py:53: FutureWarning: Dataset 'c4' is deprecated and will be deleted. Use 'allenai/c4' instead.\n",
      "  warnings.warn(\n",
      "/home/user/miniconda3/envs/bert24/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 32, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "Converting train_small to MDS format...\n",
      "train_small: 100%|████████████████████| 100000/100000 [00:12<00:00, 7947.34it/s]\n",
      "/home/user/.cache/huggingface/modules/datasets_modules/datasets/c4/584d57ebe81c209b6c7f31727066d2c4b4bba37cb7092cdd83083d5ec11207db/c4.py:53: FutureWarning: Dataset 'c4' is deprecated and will be deleted. Use 'allenai/c4' instead.\n",
      "  warnings.warn(\n",
      "Converting val_small to MDS format...\n",
      "val_small: 100%|████████████████████████| 10000/10000 [00:02<00:00, 4574.10it/s]\n"
     ]
    }
   ],
   "source": [
    "!python src/convert_dataset.py \\\n",
    "  --dataset c4 \\\n",
    "  --data_subset en \\\n",
    "  --splits train_small val_small \\\n",
    "  --out_root ./c4_small "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d2e54eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The repository for c4 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/c4.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N] ^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/MoEBERT/src/convert_dataset.py\", line 439, in <module>\n",
      "    main(parse_args())\n",
      "  File \"/home/user/MoEBERT/src/convert_dataset.py\", line 396, in main\n",
      "    dataset = build_hf_dataset(\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/MoEBERT/src/convert_dataset.py\", line 275, in build_hf_dataset\n",
      "    dataset = NoConcatDataset(dataset_name=dataset_name, data_subset=data_subset, split=split)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/MoEBERT/src/convert_dataset.py\", line 150, in __init__\n",
      "    self.hf_dataset = hf_datasets.load_dataset(path=dataset_name, name=data_subset, split=split, streaming=True)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/miniconda3/envs/bert24/lib/python3.11/site-packages/datasets/load.py\", line 2606, in load_dataset\n",
      "    builder_instance = load_dataset_builder(\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/miniconda3/envs/bert24/lib/python3.11/site-packages/datasets/load.py\", line 2277, in load_dataset_builder\n",
      "    dataset_module = dataset_module_factory(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/miniconda3/envs/bert24/lib/python3.11/site-packages/datasets/load.py\", line 1896, in dataset_module_factory\n",
      "    ).get_module()\n",
      "      ^^^^^^^^^^^^\n",
      "  File \"/home/user/miniconda3/envs/bert24/lib/python3.11/site-packages/datasets/load.py\", line 1532, in get_module\n",
      "    trust_remote_code = resolve_trust_remote_code(self.trust_remote_code, self.name)\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/miniconda3/envs/bert24/lib/python3.11/site-packages/datasets/load.py\", line 121, in resolve_trust_remote_code\n",
      "    answer = input(\n",
      "             ^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python MoEBERT/src/convert_dataset.py \\\n",
    "  --dataset c4 \\\n",
    "  --data_subset realnewslike \\\n",
    "  --splits train val \\\n",
    "  --out_root ./c4_realnewslike\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
